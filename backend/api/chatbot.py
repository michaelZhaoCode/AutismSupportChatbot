"""
chatbot.py

This module provides a Chatbot class for handling chat interactions, adding PDF files, and populating PDF files from a
directory. The Chatbot class leverages clustering and embedding functionalities to enhance responses using Cohere's
embedding service.

"""

import os
from cohere import Client

from algos.cluster import compute_cluster, give_closest_cluster
from db_funcs.file_storage import PDFStorageInterface
from db_funcs.chat_history import ChatHistoryInterface
from db_funcs.cluster_storage import ClusterStorageInterface
from utils import extract_text


class Chatbot:
    """
    A class to handle chat interactions, add PDF files, and populate PDF files from a directory.
    This class leverages clustering and embedding functionalities to enhance responses.

    Attributes:
        pdf_storage (PDFStorageInterface): Interface for PDF storage operations.
        chat_history (ChatHistoryInterface): Interface for chat history operations.
        cluster_storage (ClusterStorageInterface): Interface for cluster storage operations.
        co (Client): Cohere client for generating embeddings and chat responses.
    """

    def __init__(
            self,
            pdf_storage: PDFStorageInterface,
            chat_history: ChatHistoryInterface,
            cluster_storage: ClusterStorageInterface,
            co: Client
    ):
        """
        Initializes the Chatbot with the given interfaces and Cohere client.

        Args:
            pdf_storage (PDFStorageInterface): Interface for PDF storage operations.
            chat_history (ChatHistoryInterface): Interface for chat history operations.
            cluster_storage (ClusterStorageInterface): Interface for cluster storage operations.
            co (Client): Cohere client for generating embeddings and chat responses.
        """
        self.pdf_storage = pdf_storage
        self.chat_history = chat_history
        self.cluster_storage = cluster_storage
        self.co = co

    @staticmethod
    def _load_prompt(user_type: str, response_type: str) -> str:
        """
        Load and return a corresponding prompt from a text file based on the user type and response type.

        Args:
            user_type (str): The type of user. Must be one of 'child', 'adult', or 'researcher'.
            response_type (str): The type of response format. Must be either 'rag' or 'normal'.

        Returns:
            str: The content of the corresponding prompt file.

        Raises:
            ValueError: If the user type is not one of 'child', 'adult', or 'researcher'.
            FileNotFoundError: If the corresponding prompt file does not exist.
        """
        valid_user_types = ['child', 'adult', 'researcher']

        if user_type not in valid_user_types:
            raise ValueError(f"Invalid user type. Expected one of {valid_user_types}, got '{user_type}'.")

        current_dir = os.path.dirname(os.path.abspath(__file__))
        file_path = os.path.join(current_dir, 'prompts', response_type, f"{user_type}.txt")

        if not os.path.exists(file_path):
            raise FileNotFoundError(f"The prompt file for user type '{user_type}' does not exist at '{file_path}'.")

        with open(file_path, 'r', encoding='utf-8') as file:
            prompt = file.read()

        print(f"Found prompt for usertype: {user_type} with response type: {response_type}")

        return prompt

    def _generate_rag(self, prompt: str, username: str, usertype: str) -> str:
        """
        Generate a chat response using retrieval-augmented generation based on the given prompt and user's chat history.

        Args:
            prompt (str): The user's prompt to the chatbot.
            username (str): The username of the user interacting with the chatbot.
            usertype (str): The type of user interacting with the chatbot.

        Returns:
            str: The chat response generated by the Cohere model.
        """
        closest_files = give_closest_cluster(prompt, self.co, self.cluster_storage)
        files_content = self.pdf_storage.retrieve_pdfs(closest_files)
        chat_history = self.chat_history.retrieve_chat_history(username)
        texts = [extract_text(data) for data in files_content]
        documents = [{'title': closest_files[i], 'snippet': texts[i]} for i in range(len(closest_files))]
        prompt_format = self._load_prompt(usertype.lower(), 'rag').format(username, prompt)
        print("Getting RAG response")
        response = self.co.chat(
            model="command-r-plus",
            message=prompt_format,
            documents=documents,
            chat_history=chat_history
        )
        return response.text

    def _generate_normal(self, prompt: str, username: str, usertype: str) -> str:
        """
        Generate a normal chat response based on the given prompt and user's chat history.

        Args:
            prompt (str): The user's prompt to the chatbot.
            username (str): The username of the user interacting with the chatbot.
            usertype (str): The type of user interacting with the chatbot.

        Returns:
            str: The chat response generated by the Cohere model.
        """
        chat_history = self.chat_history.retrieve_chat_history(username)
        prompt_format = self._load_prompt(usertype.lower(), 'normal').format(username, prompt)
        print("Getting normal response")
        response = self.co.chat(
            model="command-r-plus",
            message=prompt_format,
            chat_history=chat_history
        )
        return response.text

    def _classify(self, prompt):
        """
        Classify the type of response needed based on the content of the prompt.

        Args:
            prompt (str): The user's prompt to the chatbot.

        Returns:
            str: The type of response format ('rag' or 'normal').
        """
        if 'autism' in prompt.lower():
            print("Using RAG chatbot.")
            return 'rag'
        else:
            print("Using normal chatbot.")
            return 'normal'

    def chat(self, prompt: str, username: str, usertype: str) -> str:
        """
        Generate a chat response based on the given prompt and user's chat history.

        Args:
            prompt (str): The user's prompt to the chatbot.
            username (str): The username of the user interacting with the chatbot.
            usertype (str): The type of user interacting with the chatbot.

        Returns:
            str: The chat response generated by the Cohere model.
        """
        choice = self._classify(prompt)

        response = ""

        if choice == 'rag':
            response = self._generate_rag(prompt, username, usertype)
        elif choice == 'normal':
            response = self._generate_normal(prompt, username, usertype)

        self.chat_history.insert_chat_history(username, [[prompt, response]])
        return response

    def add_pdf(self, pdf_path: str, pdf_name: str) -> None:
        """
        Add a PDF file to the database and update the cluster.

        Args:
            pdf_path (str): The path to the PDF file.
            pdf_name (str): The name to assign to the stored PDF file.
        """
        self.pdf_storage.store_pdf(pdf_path, pdf_name)
        compute_cluster([pdf_name], self.co, self.cluster_storage, self.pdf_storage)

    def populate_pdfs(self, directory_path: str) -> None:
        """
        Add multiple PDF files from a directory to the database and update the cluster.

        Args:
            directory_path (str): The path to the directory containing PDF files.
        """
        files_list = [(filename, os.path.join(directory_path, filename)) for filename in os.listdir(directory_path)]
        self.pdf_storage.bulk_insert_pdf(files_list)
        compute_cluster([file_pair[0] for file_pair in files_list], self.co, self.cluster_storage, self.pdf_storage)

    def clear_history(self, username: str) -> None:
        """Clear the chat history for a given username."""
        self.chat_history.clear_chat_history(username)
