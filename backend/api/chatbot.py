"""
chatbot.py

This module provides a Chatbot class for handling chat interactions, adding PDF files, and populating PDF
files from a directory. The Chatbot class leverages clustering and embedding functionalities to
enhance responses using the BotService.
"""

import os

from constants import MAIN_MODEL_USE
from api.botservice import BotService
from api.servicehandler import ServiceHandler
from algos.cluster import compute_cluster, give_closest_cluster
from db_funcs.file_storage import PDFStorageInterface
from db_funcs.chat_history import ChatHistoryInterface
from db_funcs.cluster_storage import ClusterStorageInterface
from utils import extract_text, chunk_pdf_in_memory


class Chatbot:
    """
    A class to handle chat interactions, add PDF files, and populate PDF files from a directory.
    This class leverages clustering and embedding functionalities to enhance responses.

    Attributes:
        pdf_storage (PDFStorageInterface): Interface for PDF storage operations.
        chat_history (ChatHistoryInterface): Interface for chat history operations.
        cluster_storage (ClusterStorageInterface): Interface for cluster storage operations.
        botservice (BotService): BotService instance for generating embeddings and chat responses.
    """

    def __init__(
            self,
            pdf_storage: PDFStorageInterface,
            chat_history: ChatHistoryInterface,
            cluster_storage: ClusterStorageInterface,
            botservice: BotService,
            service_handler: ServiceHandler
    ):
        """
        Initializes the Chatbot with the given interfaces and BotService instance.

        Args:
            pdf_storage (PDFStorageInterface): Interface for PDF storage operations.
            chat_history (ChatHistoryInterface): Interface for chat history operations.
            cluster_storage (ClusterStorageInterface): Interface for cluster storage operations.
            botservice (BotService): BotService instance for generating embeddings and chat responses.
        """
        self.pdf_storage = pdf_storage
        self.chat_history = chat_history
        self.cluster_storage = cluster_storage
        self.botservice = botservice
        self.service_handler = service_handler

    @staticmethod
    def _load_prompt(user_type: str, response_type: str) -> str:
        """
        Load and return a corresponding prompt from a text file based on the user type and response type.

        Args:
            user_type (str): The type of user. Must be one of 'child', 'adult', or 'researcher'.
            response_type (str): The type of response format. Must be either 'rag', 'normal', or 'filter'.

        Returns:
            str: The content of the corresponding prompt file.

        Raises:
            ValueError: If the user type is not one of 'child', 'adult', or 'researcher'.
            FileNotFoundError: If the corresponding prompt file does not exist.
        """
        valid_user_types = ['child', 'adult', 'researcher']

        if user_type not in valid_user_types:
            raise ValueError(f"Invalid user type. Expected one of {valid_user_types}, got '{user_type}'.")

        current_dir = os.path.dirname(os.path.abspath(__file__))
        if response_type == "filter":
            file_path = os.path.join(current_dir, 'prompts', response_type, f"prompt.txt")
        else:
            file_path = os.path.join(current_dir, 'prompts', response_type, f"{user_type}.txt")
        # TODO: add logging

        if not os.path.exists(file_path):
            raise FileNotFoundError(f"The prompt file for user type '{user_type}' does not exist at '{file_path}'.")

        with open(file_path, 'r', encoding='utf-8') as file:
            prompt = file.read()

        print(f"Found prompt for usertype: {user_type} with response type: {response_type}")

        return prompt

    def _generate(self, prompt: str, username: str, usertype: str, response_type: str) -> str:
        """
        Generate a chat response using retrieval-augmented generation based on the given prompt and user's chat history.

        Args:
            prompt (str): The user's prompt to the chatbot.
            username (str): The username of the user interacting with the chatbot.
            usertype (str): The type of user interacting with the chatbot.
            response_type (str): The type of response format to use ('rag', 'normal', or 'filter').

        Returns:
            str: The chat response generated by the BotService.
        """
        chat_history = self.chat_history.retrieve_chat_history(username)
        prompt_format = self._load_prompt(usertype.lower(), response_type).format(username, prompt)
        params = {
            "model": MAIN_MODEL_USE,
            "message": prompt_format,
            "chat_history": chat_history
        }

        if response_type == "rag":
            closest_files = give_closest_cluster(prompt, self.botservice, self.cluster_storage)
            files_content = self.pdf_storage.retrieve_pdfs(closest_files)
            texts = [extract_text(data) for data in files_content]
            documents = [{'title': closest_files[i], 'contents': texts[i]} for i in range(len(closest_files))]
            params["documents"] = documents

        print(f"Getting {response_type} response")
        response = self.botservice.chat(**params)
        return response

    def _classify(self, prompt):
        """
        Classify the type of response needed based on the content of the prompt.

        Args:
            prompt (str): The user's prompt to the chatbot.

        Returns:
            str: The type of response format ('rag', 'normal', or 'filter').
        """
        options = [
            "A specialized chatbot only meant to talk about autism related subjects",
            "A helpful chatbot for light topics and normal questions or discussion.",
            "A specialized chatbot that is only trained to deal with the user having thoughts of self-harm.",
            "A chatbot that has information about local services that the user wants to access."
        ]

        selected = self.botservice.choose(
            model=MAIN_MODEL_USE,
            query=f"Given the following user message, who should the user chat with?\nUser message: {prompt}",
            options=options,
        )[0]

        # DEBUGGING PURPOSES
        # print(selected)

        if 'autism' in selected.lower():
            print("Using RAG chatbot.")
            return 'rag'
        elif "self-harm" in selected.lower():
            print("Using filter chatbot.")
            return 'filter'
        elif "service" in selected.lower():
            print("Using service chatbot.")
            return 'service'
        else:
            print("Using normal chatbot.")
            return 'normal'

    def chat(self, prompt: str, username: str, usertype: str, location: str = "", region_id: int = -1) -> str:
        """
        Generate a chat response based on the given prompt and user's chat history, with optional location and regional context.

        Args:
            prompt (str): The user's prompt to the chatbot.
            username (str): The username of the user interacting with the chatbot.
            usertype (str): The type of user interacting with the chatbot.
            location (str, optional): The user's location, used to refine responses based on proximity. Defaults to an empty string.
            region_id (str, optional): A specified region id to provide additional regional context to the chatbot's response. Defaults to an empty string.

        Returns:
            str: The chat response generated by the BotService.
        """
        # TODO: have vector similarity comparison with database of commonly asked questions
        # TODO: add logging

        choice = self._classify(prompt)

        if choice == "service":
            response = self.service_handler.get_response(prompt, location, region_id)
        else:
            response = self._generate(prompt, username, usertype, choice).replace("**", "")

        self.chat_history.insert_chat_history(username, [[prompt, response]])
        return response

    def add_pdf(self, pdf_path: str) -> None:
        """
        Add a PDF file to the database and update the cluster.

        Args:
            pdf_path (str): The path to the PDF file.
        """
        # TODO: add logging

        chunks = chunk_pdf_in_memory(pdf_path)
        for chunk_name, chunk_content in chunks:
            self.pdf_storage.store_pdf_chunk(chunk_name, chunk_content)
        compute_cluster(
            files_list=[chunk[0] for chunk in chunks],
            botservice=self.botservice,
            cluster_storage=self.cluster_storage,
            pdf_storage=self.pdf_storage
        )

    def populate_pdfs(self, directory_path: str) -> None:
        """
        Add multiple PDF files from a directory to the database and update the cluster.

        Args:
            directory_path (str): The path to the directory containing PDF files.
        """
        # TODO: add logging

        files_list = [os.path.join(directory_path, filename) for filename in os.listdir(directory_path)]
        all_chunks = []
        for path in files_list:
            chunks = chunk_pdf_in_memory(path)
            for chunk_name, chunk_content in chunks:
                self.pdf_storage.store_pdf_chunk(chunk_name, chunk_content)
                all_chunks.append(chunk_name)
        compute_cluster(
            files_list=all_chunks,
            botservice=self.botservice,
            cluster_storage=self.cluster_storage,
            pdf_storage=self.pdf_storage
        )

    def clear_history(self, username: str) -> None:
        """Clear the chat history for a given username."""
        self.chat_history.clear_chat_history(username)
